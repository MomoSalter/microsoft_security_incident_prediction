{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3a93cc",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9444e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_parquet('data/processed/train_data.parquet')\n",
    "test_data  = pd.read_parquet('data/processed/test_data.parquet')\n",
    "validation_data  = pd.read_parquet('data/processed/validation_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e889fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IncidentId</th>\n",
       "      <th>IncidentGrade</th>\n",
       "      <th>evidence_count</th>\n",
       "      <th>DetectorId_nunique</th>\n",
       "      <th>AlertTitle_nunique</th>\n",
       "      <th>DeviceId_nunique</th>\n",
       "      <th>Sha256_nunique</th>\n",
       "      <th>IpAddress_nunique</th>\n",
       "      <th>Url_nunique</th>\n",
       "      <th>AccountSid_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>Roles_Compromised</th>\n",
       "      <th>Roles_Contextual</th>\n",
       "      <th>Roles_Destination</th>\n",
       "      <th>Roles_Other</th>\n",
       "      <th>Roles_Source</th>\n",
       "      <th>Roles_Suspicious</th>\n",
       "      <th>org_rate_BenignPositive</th>\n",
       "      <th>org_rate_FalsePositive</th>\n",
       "      <th>org_rate_TruePositive</th>\n",
       "      <th>org_incident_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TruePositive</td>\n",
       "      <td>29997</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>874</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>0.990546</td>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>BenignPositive</td>\n",
       "      <td>20525</td>\n",
       "      <td>113</td>\n",
       "      <td>934</td>\n",
       "      <td>11</td>\n",
       "      <td>1881</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.986812</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TruePositive</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.274071</td>\n",
       "      <td>0.722551</td>\n",
       "      <td>511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>BenignPositive</td>\n",
       "      <td>12252</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2474</td>\n",
       "      <td>3</td>\n",
       "      <td>1721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.846876</td>\n",
       "      <td>1177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>TruePositive</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217158</td>\n",
       "      <td>0.719717</td>\n",
       "      <td>0.063125</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IncidentId   IncidentGrade  evidence_count  DetectorId_nunique  \\\n",
       "0           0    TruePositive           29997                   6   \n",
       "1           2  BenignPositive           20525                 113   \n",
       "2           3    TruePositive               3                   1   \n",
       "3           7  BenignPositive           12252                   8   \n",
       "4           8    TruePositive               6                   2   \n",
       "\n",
       "   AlertTitle_nunique  DeviceId_nunique  Sha256_nunique  IpAddress_nunique  \\\n",
       "0                   6                 1               1                874   \n",
       "1                 934                11            1881                  3   \n",
       "2                   1                 1               1                  2   \n",
       "3                  19                 3               3               2474   \n",
       "4                   2                 1               1                  1   \n",
       "\n",
       "   Url_nunique  AccountSid_nunique  ...  Roles_Compromised  Roles_Contextual  \\\n",
       "0            1                 148  ...                0.0               0.0   \n",
       "1            3                   7  ...                0.0               0.0   \n",
       "2            1                   2  ...                0.0               1.0   \n",
       "3            3                1721  ...                0.0               1.0   \n",
       "4            1                   3  ...                0.0               0.0   \n",
       "\n",
       "   Roles_Destination  Roles_Other  Roles_Source  Roles_Suspicious  \\\n",
       "0                0.0          0.0           0.0               0.0   \n",
       "1                0.0          0.0           0.0               0.0   \n",
       "2                0.0          0.0           0.0               0.0   \n",
       "3                0.0          0.0           0.0               0.0   \n",
       "4                0.0          0.0           0.0               0.0   \n",
       "\n",
       "   org_rate_BenignPositive  org_rate_FalsePositive  org_rate_TruePositive  \\\n",
       "0                 0.000089                0.009365               0.990546   \n",
       "1                 0.986812                0.013188               0.000000   \n",
       "2                 0.003378                0.274071               0.722551   \n",
       "3                 0.153124                0.000000               0.846876   \n",
       "4                 0.217158                0.719717               0.063125   \n",
       "\n",
       "   org_incident_count  \n",
       "0               264.0  \n",
       "1               163.0  \n",
       "2               511.0  \n",
       "3              1177.0  \n",
       "4               269.0  \n",
       "\n",
       "[5 rows x 178 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1e6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[\"label\"] = label_encoder.fit_transform(train_data[\"IncidentGrade\"])\n",
    "validation_data[\"label\"] = label_encoder.transform(validation_data[\"IncidentGrade\"])\n",
    "test_data[\"label\"] = label_encoder.transform(test_data[\"IncidentGrade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b688cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BenignPositive', 'FalsePositive', 'TruePositive'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e94da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    196317\n",
       "1    121642\n",
       "2     86051\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e533a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(['IncidentId', 'IncidentGrade', 'label'], axis=1).values\n",
    "y_train = train_data[\"label\"].values\n",
    "\n",
    "X_validation = validation_data.drop(['IncidentId', 'IncidentGrade', 'label'], axis=1).values\n",
    "y_validation = validation_data[\"label\"].values\n",
    "\n",
    "X_test = test_data.drop(['IncidentId', 'IncidentGrade', 'label'], axis=1).values\n",
    "y_test = test_data[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db8d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape : (404010, 176)\n",
      "validation shape : (44891, 176)\n",
      "testing shape : (236267, 176)\n"
     ]
    }
   ],
   "source": [
    "print(f\"training shape : {X_train.shape}\")\n",
    "print(f\"validation shape : {X_validation.shape}\")\n",
    "print(f\"testing shape : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c11b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "749c309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class_counts  = np.bincount(y_train)\n",
    "class_weights = len(y_train) / (len(class_counts) * class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd488b6",
   "metadata": {},
   "source": [
    "## prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f102fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c042f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce659e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.long)\n",
    ")\n",
    "validation_dataset = TensorDataset(\n",
    "    torch.tensor(X_validation, dtype=torch.float32),\n",
    "    torch.tensor(y_validation, dtype=torch.long)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.long)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=4096)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026cbbb7",
   "metadata": {},
   "source": [
    "## create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70926be6",
   "metadata": {},
   "source": [
    "### create the residual blocks class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54a3998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block=nn.Sequential(\n",
    "            nn.BatchNorm1d(in_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "        self.shourtcut = (\n",
    "            nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x) + self.shourtcut(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff0182",
   "metadata": {},
   "source": [
    "### create the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e1340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualModel(nn.Module):\n",
    "    def __init__(self, in_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.input= nn.Linear(in_dim, 512)\n",
    "        self.blocks = nn.Sequential(\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 256),\n",
    "            ResidualBlock(256, 256),\n",
    "            ResidualBlock(256, 128),\n",
    "            ResidualBlock(128, 128)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd620e10",
   "metadata": {},
   "source": [
    "### create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acfdb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "num_classes = 3\n",
    "\n",
    "model = ResidualModel(input_dim, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d1ac3",
   "metadata": {},
   "source": [
    "### print the number of leanable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12022d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197827"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(parameter.numel() for parameter in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b031882",
   "metadata": {},
   "source": [
    "## create the loss, optimizer and the learning rate decay scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bb0e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cross entropy loss with class weight\n",
    "critertion = nn.CrossEntropyLoss(\n",
    "    weight= torch.tensor(class_weights, dtype=torch.float32).to(device),\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "# create adamw optimizer with a soft weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "\n",
    "# create learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=20,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe93cb",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1895512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 0.5888 | Val Loss: 0.5678 | Val Macro-F1: 0.8470\n",
      ">>>> New best saved (val macro-F1: 0.8470)\n",
      "Epoch 002 | Train Loss: 0.5568 | Val Loss: 0.5599 | Val Macro-F1: 0.8477\n",
      ">>>> New best saved (val macro-F1: 0.8477)\n",
      "Epoch 003 | Train Loss: 0.5434 | Val Loss: 0.5483 | Val Macro-F1: 0.8550\n",
      ">>>> New best saved (val macro-F1: 0.8550)\n",
      "Epoch 004 | Train Loss: 0.5319 | Val Loss: 0.5402 | Val Macro-F1: 0.8647\n",
      ">>>> New best saved (val macro-F1: 0.8647)\n",
      "Epoch 005 | Train Loss: 0.5256 | Val Loss: 0.5340 | Val Macro-F1: 0.8699\n",
      ">>>> New best saved (val macro-F1: 0.8699)\n",
      "Epoch 006 | Train Loss: 0.5209 | Val Loss: 0.5264 | Val Macro-F1: 0.8714\n",
      ">>>> New best saved (val macro-F1: 0.8714)\n",
      "Epoch 007 | Train Loss: 0.5173 | Val Loss: 0.5216 | Val Macro-F1: 0.8762\n",
      ">>>> New best saved (val macro-F1: 0.8762)\n",
      "Epoch 008 | Train Loss: 0.5122 | Val Loss: 0.5174 | Val Macro-F1: 0.8779\n",
      ">>>> New best saved (val macro-F1: 0.8779)\n",
      "Epoch 009 | Train Loss: 0.5089 | Val Loss: 0.5198 | Val Macro-F1: 0.8692\n",
      "Epoch 010 | Train Loss: 0.5062 | Val Loss: 0.5136 | Val Macro-F1: 0.8773\n",
      "Epoch 011 | Train Loss: 0.5029 | Val Loss: 0.5113 | Val Macro-F1: 0.8832\n",
      ">>>> New best saved (val macro-F1: 0.8832)\n",
      "Epoch 012 | Train Loss: 0.5010 | Val Loss: 0.5121 | Val Macro-F1: 0.8755\n",
      "Epoch 013 | Train Loss: 0.4991 | Val Loss: 0.5116 | Val Macro-F1: 0.8807\n",
      "Epoch 014 | Train Loss: 0.4975 | Val Loss: 0.5119 | Val Macro-F1: 0.8831\n",
      "Epoch 015 | Train Loss: 0.4947 | Val Loss: 0.5084 | Val Macro-F1: 0.8829\n",
      "Epoch 016 | Train Loss: 0.4926 | Val Loss: 0.5065 | Val Macro-F1: 0.8830\n",
      "Epoch 017 | Train Loss: 0.4911 | Val Loss: 0.5048 | Val Macro-F1: 0.8799\n",
      "Epoch 018 | Train Loss: 0.4884 | Val Loss: 0.4979 | Val Macro-F1: 0.8881\n",
      ">>>> New best saved (val macro-F1: 0.8881)\n",
      "Epoch 019 | Train Loss: 0.4875 | Val Loss: 0.4951 | Val Macro-F1: 0.8894\n",
      ">>>> New best saved (val macro-F1: 0.8894)\n",
      "Epoch 020 | Train Loss: 0.4855 | Val Loss: 0.4939 | Val Macro-F1: 0.8882\n",
      "Epoch 021 | Train Loss: 0.4839 | Val Loss: 0.4919 | Val Macro-F1: 0.8929\n",
      ">>>> New best saved (val macro-F1: 0.8929)\n",
      "Epoch 022 | Train Loss: 0.4826 | Val Loss: 0.4943 | Val Macro-F1: 0.8933\n",
      ">>>> New best saved (val macro-F1: 0.8933)\n",
      "Epoch 023 | Train Loss: 0.4815 | Val Loss: 0.4947 | Val Macro-F1: 0.8942\n",
      ">>>> New best saved (val macro-F1: 0.8942)\n",
      "Epoch 024 | Train Loss: 0.4808 | Val Loss: 0.4923 | Val Macro-F1: 0.8911\n",
      "Epoch 025 | Train Loss: 0.4784 | Val Loss: 0.4946 | Val Macro-F1: 0.8890\n",
      "Epoch 026 | Train Loss: 0.4772 | Val Loss: 0.4910 | Val Macro-F1: 0.8889\n",
      "Epoch 027 | Train Loss: 0.4769 | Val Loss: 0.4921 | Val Macro-F1: 0.8868\n",
      "Epoch 028 | Train Loss: 0.4753 | Val Loss: 0.4866 | Val Macro-F1: 0.8958\n",
      ">>>> New best saved (val macro-F1: 0.8958)\n",
      "Epoch 029 | Train Loss: 0.4741 | Val Loss: 0.4924 | Val Macro-F1: 0.8910\n",
      "Epoch 030 | Train Loss: 0.4732 | Val Loss: 0.4871 | Val Macro-F1: 0.8947\n",
      "Epoch 031 | Train Loss: 0.4721 | Val Loss: 0.4903 | Val Macro-F1: 0.8924\n",
      "Epoch 032 | Train Loss: 0.4707 | Val Loss: 0.4861 | Val Macro-F1: 0.8968\n",
      ">>>> New best saved (val macro-F1: 0.8968)\n",
      "Epoch 033 | Train Loss: 0.4699 | Val Loss: 0.4856 | Val Macro-F1: 0.8960\n",
      "Epoch 034 | Train Loss: 0.4684 | Val Loss: 0.4939 | Val Macro-F1: 0.8908\n",
      "Epoch 035 | Train Loss: 0.4673 | Val Loss: 0.4940 | Val Macro-F1: 0.8906\n",
      "Epoch 036 | Train Loss: 0.4674 | Val Loss: 0.4876 | Val Macro-F1: 0.8894\n",
      "Epoch 037 | Train Loss: 0.4656 | Val Loss: 0.4792 | Val Macro-F1: 0.9018\n",
      ">>>> New best saved (val macro-F1: 0.9018)\n",
      "Epoch 038 | Train Loss: 0.4646 | Val Loss: 0.4957 | Val Macro-F1: 0.8868\n",
      "Epoch 039 | Train Loss: 0.4636 | Val Loss: 0.4876 | Val Macro-F1: 0.8966\n",
      "Epoch 040 | Train Loss: 0.4630 | Val Loss: 0.4767 | Val Macro-F1: 0.9018\n",
      ">>>> New best saved (val macro-F1: 0.9018)\n",
      "Epoch 041 | Train Loss: 0.4619 | Val Loss: 0.4787 | Val Macro-F1: 0.9018\n",
      "Epoch 042 | Train Loss: 0.4616 | Val Loss: 0.4758 | Val Macro-F1: 0.9031\n",
      ">>>> New best saved (val macro-F1: 0.9031)\n",
      "Epoch 043 | Train Loss: 0.4607 | Val Loss: 0.4854 | Val Macro-F1: 0.8943\n",
      "Epoch 044 | Train Loss: 0.4596 | Val Loss: 0.4863 | Val Macro-F1: 0.8944\n",
      "Epoch 045 | Train Loss: 0.4582 | Val Loss: 0.4765 | Val Macro-F1: 0.9033\n",
      ">>>> New best saved (val macro-F1: 0.9033)\n",
      "Epoch 046 | Train Loss: 0.4578 | Val Loss: 0.4809 | Val Macro-F1: 0.9006\n",
      "Epoch 047 | Train Loss: 0.4572 | Val Loss: 0.4827 | Val Macro-F1: 0.8964\n",
      "Epoch 048 | Train Loss: 0.4562 | Val Loss: 0.4762 | Val Macro-F1: 0.9018\n",
      "Epoch 049 | Train Loss: 0.4555 | Val Loss: 0.4782 | Val Macro-F1: 0.9052\n",
      ">>>> New best saved (val macro-F1: 0.9052)\n",
      "Epoch 050 | Train Loss: 0.4542 | Val Loss: 0.4775 | Val Macro-F1: 0.9030\n",
      "Epoch 051 | Train Loss: 0.4534 | Val Loss: 0.4722 | Val Macro-F1: 0.9068\n",
      ">>>> New best saved (val macro-F1: 0.9068)\n",
      "Epoch 052 | Train Loss: 0.4524 | Val Loss: 0.4811 | Val Macro-F1: 0.8988\n",
      "Epoch 053 | Train Loss: 0.4516 | Val Loss: 0.4936 | Val Macro-F1: 0.8927\n",
      "Epoch 054 | Train Loss: 0.4518 | Val Loss: 0.4862 | Val Macro-F1: 0.8987\n",
      "Epoch 055 | Train Loss: 0.4506 | Val Loss: 0.4738 | Val Macro-F1: 0.9061\n",
      "Epoch 056 | Train Loss: 0.4492 | Val Loss: 0.5102 | Val Macro-F1: 0.8821\n",
      "Epoch 057 | Train Loss: 0.4495 | Val Loss: 0.4813 | Val Macro-F1: 0.8987\n",
      "Epoch 058 | Train Loss: 0.4491 | Val Loss: 0.4778 | Val Macro-F1: 0.9030\n",
      "Epoch 059 | Train Loss: 0.4475 | Val Loss: 0.4820 | Val Macro-F1: 0.8992\n",
      "Epoch 060 | Train Loss: 0.4466 | Val Loss: 0.5098 | Val Macro-F1: 0.8838\n",
      "Epoch 061 | Train Loss: 0.4462 | Val Loss: 0.4872 | Val Macro-F1: 0.8967\n",
      "Epoch 062 | Train Loss: 0.4454 | Val Loss: 0.4760 | Val Macro-F1: 0.9044\n",
      "Epoch 063 | Train Loss: 0.4448 | Val Loss: 0.4813 | Val Macro-F1: 0.9012\n",
      "Epoch 064 | Train Loss: 0.4442 | Val Loss: 0.4791 | Val Macro-F1: 0.9036\n",
      "Epoch 065 | Train Loss: 0.4434 | Val Loss: 0.4814 | Val Macro-F1: 0.8993\n",
      "Epoch 066 | Train Loss: 0.4426 | Val Loss: 0.4755 | Val Macro-F1: 0.9052\n",
      "Epoch 067 | Train Loss: 0.4421 | Val Loss: 0.4791 | Val Macro-F1: 0.9023\n",
      "Epoch 068 | Train Loss: 0.4409 | Val Loss: 0.4720 | Val Macro-F1: 0.9079\n",
      ">>>> New best saved (val macro-F1: 0.9079)\n",
      "Epoch 069 | Train Loss: 0.4400 | Val Loss: 0.4758 | Val Macro-F1: 0.9054\n",
      "Epoch 070 | Train Loss: 0.4393 | Val Loss: 0.4744 | Val Macro-F1: 0.9067\n",
      "Epoch 071 | Train Loss: 0.4383 | Val Loss: 0.4768 | Val Macro-F1: 0.9038\n",
      "Epoch 072 | Train Loss: 0.4383 | Val Loss: 0.4852 | Val Macro-F1: 0.9000\n",
      "Epoch 073 | Train Loss: 0.4373 | Val Loss: 0.4726 | Val Macro-F1: 0.9082\n",
      ">>>> New best saved (val macro-F1: 0.9082)\n",
      "Epoch 074 | Train Loss: 0.4365 | Val Loss: 0.4765 | Val Macro-F1: 0.9072\n",
      "Epoch 075 | Train Loss: 0.4359 | Val Loss: 0.4721 | Val Macro-F1: 0.9084\n",
      ">>>> New best saved (val macro-F1: 0.9084)\n",
      "Epoch 076 | Train Loss: 0.4348 | Val Loss: 0.4756 | Val Macro-F1: 0.9054\n",
      "Epoch 077 | Train Loss: 0.4340 | Val Loss: 0.4835 | Val Macro-F1: 0.9006\n",
      "Epoch 078 | Train Loss: 0.4336 | Val Loss: 0.4736 | Val Macro-F1: 0.9086\n",
      ">>>> New best saved (val macro-F1: 0.9086)\n",
      "Epoch 079 | Train Loss: 0.4330 | Val Loss: 0.4745 | Val Macro-F1: 0.9059\n",
      "Epoch 080 | Train Loss: 0.4325 | Val Loss: 0.4759 | Val Macro-F1: 0.9061\n",
      "Epoch 081 | Train Loss: 0.4315 | Val Loss: 0.4733 | Val Macro-F1: 0.9078\n",
      "Epoch 082 | Train Loss: 0.4306 | Val Loss: 0.4731 | Val Macro-F1: 0.9099\n",
      ">>>> New best saved (val macro-F1: 0.9099)\n",
      "Epoch 083 | Train Loss: 0.4301 | Val Loss: 0.4763 | Val Macro-F1: 0.9081\n",
      "Epoch 084 | Train Loss: 0.4299 | Val Loss: 0.4772 | Val Macro-F1: 0.9075\n",
      "Epoch 085 | Train Loss: 0.4290 | Val Loss: 0.4732 | Val Macro-F1: 0.9087\n",
      "Epoch 086 | Train Loss: 0.4284 | Val Loss: 0.4746 | Val Macro-F1: 0.9077\n",
      "Epoch 087 | Train Loss: 0.4272 | Val Loss: 0.4746 | Val Macro-F1: 0.9081\n",
      "Epoch 088 | Train Loss: 0.4270 | Val Loss: 0.4752 | Val Macro-F1: 0.9078\n",
      "Epoch 089 | Train Loss: 0.4261 | Val Loss: 0.4865 | Val Macro-F1: 0.9001\n",
      "Epoch 090 | Train Loss: 0.4257 | Val Loss: 0.4772 | Val Macro-F1: 0.9079\n",
      "Epoch 091 | Train Loss: 0.4254 | Val Loss: 0.4844 | Val Macro-F1: 0.9034\n",
      "Epoch 092 | Train Loss: 0.4247 | Val Loss: 0.4725 | Val Macro-F1: 0.9088\n",
      "Epoch 093 | Train Loss: 0.4236 | Val Loss: 0.4734 | Val Macro-F1: 0.9112\n",
      ">>>> New best saved (val macro-F1: 0.9112)\n",
      "Epoch 094 | Train Loss: 0.4236 | Val Loss: 0.4791 | Val Macro-F1: 0.9051\n",
      "Epoch 095 | Train Loss: 0.4230 | Val Loss: 0.4841 | Val Macro-F1: 0.9039\n",
      "Epoch 096 | Train Loss: 0.4221 | Val Loss: 0.4723 | Val Macro-F1: 0.9114\n",
      ">>>> New best saved (val macro-F1: 0.9114)\n",
      "Epoch 097 | Train Loss: 0.4217 | Val Loss: 0.4911 | Val Macro-F1: 0.8980\n",
      "Epoch 098 | Train Loss: 0.4209 | Val Loss: 0.4750 | Val Macro-F1: 0.9087\n",
      "Epoch 099 | Train Loss: 0.4199 | Val Loss: 0.4791 | Val Macro-F1: 0.9076\n",
      "Epoch 100 | Train Loss: 0.4197 | Val Loss: 0.4712 | Val Macro-F1: 0.9118\n",
      ">>>> New best saved (val macro-F1: 0.9118)\n",
      "Epoch 101 | Train Loss: 0.4196 | Val Loss: 0.4757 | Val Macro-F1: 0.9087\n",
      "Epoch 102 | Train Loss: 0.4182 | Val Loss: 0.4793 | Val Macro-F1: 0.9058\n",
      "Epoch 103 | Train Loss: 0.4176 | Val Loss: 0.4805 | Val Macro-F1: 0.9060\n",
      "Epoch 104 | Train Loss: 0.4170 | Val Loss: 0.4743 | Val Macro-F1: 0.9133\n",
      ">>>> New best saved (val macro-F1: 0.9133)\n",
      "Epoch 105 | Train Loss: 0.4163 | Val Loss: 0.4834 | Val Macro-F1: 0.9068\n",
      "Epoch 106 | Train Loss: 0.4166 | Val Loss: 0.4765 | Val Macro-F1: 0.9109\n",
      "Epoch 107 | Train Loss: 0.4156 | Val Loss: 0.4774 | Val Macro-F1: 0.9107\n",
      "Epoch 108 | Train Loss: 0.4153 | Val Loss: 0.4783 | Val Macro-F1: 0.9099\n",
      "Epoch 109 | Train Loss: 0.4148 | Val Loss: 0.4826 | Val Macro-F1: 0.9069\n",
      "Epoch 110 | Train Loss: 0.4136 | Val Loss: 0.4793 | Val Macro-F1: 0.9097\n",
      "Epoch 111 | Train Loss: 0.4136 | Val Loss: 0.4817 | Val Macro-F1: 0.9067\n",
      "Epoch 112 | Train Loss: 0.4131 | Val Loss: 0.4787 | Val Macro-F1: 0.9113\n",
      "Epoch 113 | Train Loss: 0.4128 | Val Loss: 0.4786 | Val Macro-F1: 0.9106\n",
      "Epoch 114 | Train Loss: 0.4116 | Val Loss: 0.4811 | Val Macro-F1: 0.9091\n",
      "Epoch 115 | Train Loss: 0.4110 | Val Loss: 0.4791 | Val Macro-F1: 0.9100\n",
      "Epoch 116 | Train Loss: 0.4105 | Val Loss: 0.4777 | Val Macro-F1: 0.9116\n",
      "Epoch 117 | Train Loss: 0.4100 | Val Loss: 0.4799 | Val Macro-F1: 0.9096\n",
      "Epoch 118 | Train Loss: 0.4096 | Val Loss: 0.4905 | Val Macro-F1: 0.9028\n",
      "Epoch 119 | Train Loss: 0.4098 | Val Loss: 0.4809 | Val Macro-F1: 0.9099\n",
      "Epoch 120 | Train Loss: 0.4088 | Val Loss: 0.4806 | Val Macro-F1: 0.9103\n",
      "Epoch 121 | Train Loss: 0.4083 | Val Loss: 0.4785 | Val Macro-F1: 0.9090\n",
      "Epoch 122 | Train Loss: 0.4080 | Val Loss: 0.4819 | Val Macro-F1: 0.9100\n",
      "Epoch 123 | Train Loss: 0.4072 | Val Loss: 0.4779 | Val Macro-F1: 0.9120\n",
      "Epoch 124 | Train Loss: 0.4071 | Val Loss: 0.4784 | Val Macro-F1: 0.9128\n",
      "Epoch 125 | Train Loss: 0.4065 | Val Loss: 0.4828 | Val Macro-F1: 0.9076\n",
      "Epoch 126 | Train Loss: 0.4004 | Val Loss: 0.4829 | Val Macro-F1: 0.9101\n",
      "Epoch 127 | Train Loss: 0.3987 | Val Loss: 0.4869 | Val Macro-F1: 0.9087\n",
      "Epoch 128 | Train Loss: 0.3977 | Val Loss: 0.4826 | Val Macro-F1: 0.9114\n",
      "Epoch 129 | Train Loss: 0.3972 | Val Loss: 0.4799 | Val Macro-F1: 0.9141\n",
      ">>>> New best saved (val macro-F1: 0.9141)\n",
      "Epoch 130 | Train Loss: 0.3971 | Val Loss: 0.4867 | Val Macro-F1: 0.9084\n",
      "Epoch 131 | Train Loss: 0.3968 | Val Loss: 0.5048 | Val Macro-F1: 0.8984\n",
      "Epoch 132 | Train Loss: 0.3966 | Val Loss: 0.4893 | Val Macro-F1: 0.9084\n",
      "Epoch 133 | Train Loss: 0.3958 | Val Loss: 0.4828 | Val Macro-F1: 0.9127\n",
      "Epoch 134 | Train Loss: 0.3956 | Val Loss: 0.4909 | Val Macro-F1: 0.9072\n",
      "Epoch 135 | Train Loss: 0.3953 | Val Loss: 0.4820 | Val Macro-F1: 0.9128\n",
      "Epoch 136 | Train Loss: 0.3954 | Val Loss: 0.4841 | Val Macro-F1: 0.9120\n",
      "Epoch 137 | Train Loss: 0.3944 | Val Loss: 0.4818 | Val Macro-F1: 0.9123\n",
      "Epoch 138 | Train Loss: 0.3942 | Val Loss: 0.4803 | Val Macro-F1: 0.9135\n",
      "Epoch 139 | Train Loss: 0.3944 | Val Loss: 0.4893 | Val Macro-F1: 0.9093\n",
      "Epoch 140 | Train Loss: 0.3941 | Val Loss: 0.4802 | Val Macro-F1: 0.9148\n",
      ">>>> New best saved (val macro-F1: 0.9148)\n",
      "Epoch 141 | Train Loss: 0.3938 | Val Loss: 0.4879 | Val Macro-F1: 0.9115\n",
      "Epoch 142 | Train Loss: 0.3933 | Val Loss: 0.4909 | Val Macro-F1: 0.9101\n",
      "Epoch 143 | Train Loss: 0.3934 | Val Loss: 0.4828 | Val Macro-F1: 0.9130\n",
      "Epoch 144 | Train Loss: 0.3930 | Val Loss: 0.4824 | Val Macro-F1: 0.9133\n",
      "Epoch 145 | Train Loss: 0.3927 | Val Loss: 0.4837 | Val Macro-F1: 0.9126\n",
      "Epoch 146 | Train Loss: 0.3928 | Val Loss: 0.4822 | Val Macro-F1: 0.9136\n",
      "Epoch 147 | Train Loss: 0.3926 | Val Loss: 0.4839 | Val Macro-F1: 0.9119\n",
      "Epoch 148 | Train Loss: 0.3921 | Val Loss: 0.4832 | Val Macro-F1: 0.9126\n",
      "Epoch 149 | Train Loss: 0.3923 | Val Loss: 0.4853 | Val Macro-F1: 0.9126\n",
      "Epoch 150 | Train Loss: 0.3915 | Val Loss: 0.4869 | Val Macro-F1: 0.9123\n",
      "Epoch 151 | Train Loss: 0.3915 | Val Loss: 0.4847 | Val Macro-F1: 0.9135\n",
      "Epoch 152 | Train Loss: 0.3911 | Val Loss: 0.4845 | Val Macro-F1: 0.9145\n",
      "Epoch 153 | Train Loss: 0.3906 | Val Loss: 0.4857 | Val Macro-F1: 0.9125\n",
      "Epoch 154 | Train Loss: 0.3908 | Val Loss: 0.4825 | Val Macro-F1: 0.9151\n",
      ">>>> New best saved (val macro-F1: 0.9151)\n",
      "Epoch 155 | Train Loss: 0.3902 | Val Loss: 0.4911 | Val Macro-F1: 0.9092\n",
      "Epoch 156 | Train Loss: 0.3904 | Val Loss: 0.4828 | Val Macro-F1: 0.9147\n",
      "Epoch 157 | Train Loss: 0.3898 | Val Loss: 0.4851 | Val Macro-F1: 0.9133\n",
      "Epoch 158 | Train Loss: 0.3899 | Val Loss: 0.4906 | Val Macro-F1: 0.9102\n",
      "Epoch 159 | Train Loss: 0.3897 | Val Loss: 0.4894 | Val Macro-F1: 0.9111\n",
      "Epoch 160 | Train Loss: 0.3892 | Val Loss: 0.4902 | Val Macro-F1: 0.9104\n",
      "Epoch 161 | Train Loss: 0.3892 | Val Loss: 0.4830 | Val Macro-F1: 0.9151\n",
      "Epoch 162 | Train Loss: 0.3889 | Val Loss: 0.4864 | Val Macro-F1: 0.9129\n",
      "Epoch 163 | Train Loss: 0.3888 | Val Loss: 0.4963 | Val Macro-F1: 0.9069\n",
      "Epoch 164 | Train Loss: 0.3886 | Val Loss: 0.4866 | Val Macro-F1: 0.9127\n",
      "Epoch 165 | Train Loss: 0.3888 | Val Loss: 0.4842 | Val Macro-F1: 0.9140\n",
      "Epoch 166 | Train Loss: 0.3884 | Val Loss: 0.4977 | Val Macro-F1: 0.9066\n",
      "Epoch 167 | Train Loss: 0.3883 | Val Loss: 0.4972 | Val Macro-F1: 0.9054\n",
      "Epoch 168 | Train Loss: 0.3879 | Val Loss: 0.4819 | Val Macro-F1: 0.9158\n",
      ">>>> New best saved (val macro-F1: 0.9158)\n",
      "Epoch 169 | Train Loss: 0.3879 | Val Loss: 0.4953 | Val Macro-F1: 0.9093\n",
      "Epoch 170 | Train Loss: 0.3876 | Val Loss: 0.4940 | Val Macro-F1: 0.9082\n",
      "Epoch 171 | Train Loss: 0.3874 | Val Loss: 0.4858 | Val Macro-F1: 0.9136\n",
      "Epoch 172 | Train Loss: 0.3873 | Val Loss: 0.4928 | Val Macro-F1: 0.9102\n",
      "Epoch 173 | Train Loss: 0.3867 | Val Loss: 0.4834 | Val Macro-F1: 0.9159\n",
      ">>>> New best saved (val macro-F1: 0.9159)\n",
      "Epoch 174 | Train Loss: 0.3864 | Val Loss: 0.4839 | Val Macro-F1: 0.9145\n",
      "Epoch 175 | Train Loss: 0.3861 | Val Loss: 0.4842 | Val Macro-F1: 0.9145\n",
      "Epoch 176 | Train Loss: 0.3862 | Val Loss: 0.4859 | Val Macro-F1: 0.9136\n",
      "Epoch 177 | Train Loss: 0.3866 | Val Loss: 0.4859 | Val Macro-F1: 0.9145\n",
      "Epoch 178 | Train Loss: 0.3862 | Val Loss: 0.4886 | Val Macro-F1: 0.9131\n",
      "Epoch 179 | Train Loss: 0.3859 | Val Loss: 0.4875 | Val Macro-F1: 0.9129\n",
      "Epoch 180 | Train Loss: 0.3859 | Val Loss: 0.4941 | Val Macro-F1: 0.9098\n",
      "Epoch 181 | Train Loss: 0.3858 | Val Loss: 0.4865 | Val Macro-F1: 0.9138\n",
      "Epoch 182 | Train Loss: 0.3854 | Val Loss: 0.4851 | Val Macro-F1: 0.9139\n",
      "Epoch 183 | Train Loss: 0.3856 | Val Loss: 0.4837 | Val Macro-F1: 0.9159\n",
      ">>>> New best saved (val macro-F1: 0.9159)\n",
      "Epoch 184 | Train Loss: 0.3853 | Val Loss: 0.4863 | Val Macro-F1: 0.9145\n",
      "Epoch 185 | Train Loss: 0.3851 | Val Loss: 0.4855 | Val Macro-F1: 0.9142\n",
      "Epoch 186 | Train Loss: 0.3847 | Val Loss: 0.4842 | Val Macro-F1: 0.9155\n",
      "Epoch 187 | Train Loss: 0.3846 | Val Loss: 0.4856 | Val Macro-F1: 0.9131\n",
      "Epoch 188 | Train Loss: 0.3847 | Val Loss: 0.4854 | Val Macro-F1: 0.9142\n",
      "Epoch 189 | Train Loss: 0.3843 | Val Loss: 0.4863 | Val Macro-F1: 0.9144\n",
      "Epoch 190 | Train Loss: 0.3843 | Val Loss: 0.4906 | Val Macro-F1: 0.9114\n",
      "Epoch 191 | Train Loss: 0.3844 | Val Loss: 0.4875 | Val Macro-F1: 0.9137\n",
      "Epoch 192 | Train Loss: 0.3844 | Val Loss: 0.4909 | Val Macro-F1: 0.9121\n",
      "Epoch 193 | Train Loss: 0.3839 | Val Loss: 0.4856 | Val Macro-F1: 0.9148\n",
      "Epoch 194 | Train Loss: 0.3834 | Val Loss: 0.4872 | Val Macro-F1: 0.9139\n",
      "Epoch 195 | Train Loss: 0.3814 | Val Loss: 0.4860 | Val Macro-F1: 0.9151\n",
      "Epoch 196 | Train Loss: 0.3801 | Val Loss: 0.4869 | Val Macro-F1: 0.9149\n",
      "Epoch 197 | Train Loss: 0.3795 | Val Loss: 0.4851 | Val Macro-F1: 0.9160\n",
      ">>>> New best saved (val macro-F1: 0.9160)\n",
      "Epoch 198 | Train Loss: 0.3794 | Val Loss: 0.4874 | Val Macro-F1: 0.9158\n",
      "Epoch 199 | Train Loss: 0.3793 | Val Loss: 0.4848 | Val Macro-F1: 0.9170\n",
      ">>>> New best saved (val macro-F1: 0.9170)\n",
      "Epoch 200 | Train Loss: 0.3794 | Val Loss: 0.4880 | Val Macro-F1: 0.9141\n",
      "Epoch 201 | Train Loss: 0.3792 | Val Loss: 0.4870 | Val Macro-F1: 0.9152\n",
      "Epoch 202 | Train Loss: 0.3789 | Val Loss: 0.4879 | Val Macro-F1: 0.9143\n",
      "Epoch 203 | Train Loss: 0.3789 | Val Loss: 0.4867 | Val Macro-F1: 0.9158\n",
      "Epoch 204 | Train Loss: 0.3788 | Val Loss: 0.4884 | Val Macro-F1: 0.9144\n",
      "Epoch 205 | Train Loss: 0.3787 | Val Loss: 0.4872 | Val Macro-F1: 0.9161\n",
      "Epoch 206 | Train Loss: 0.3784 | Val Loss: 0.4876 | Val Macro-F1: 0.9157\n",
      "Epoch 207 | Train Loss: 0.3784 | Val Loss: 0.4933 | Val Macro-F1: 0.9130\n",
      "Epoch 208 | Train Loss: 0.3786 | Val Loss: 0.4914 | Val Macro-F1: 0.9127\n",
      "Epoch 209 | Train Loss: 0.3785 | Val Loss: 0.4882 | Val Macro-F1: 0.9154\n",
      "Epoch 210 | Train Loss: 0.3782 | Val Loss: 0.4986 | Val Macro-F1: 0.9081\n",
      "Epoch 211 | Train Loss: 0.3783 | Val Loss: 0.5014 | Val Macro-F1: 0.9069\n",
      "Epoch 212 | Train Loss: 0.3781 | Val Loss: 0.4917 | Val Macro-F1: 0.9121\n",
      "Epoch 213 | Train Loss: 0.3779 | Val Loss: 0.5117 | Val Macro-F1: 0.9013\n",
      "Epoch 214 | Train Loss: 0.3779 | Val Loss: 0.4888 | Val Macro-F1: 0.9149\n",
      "Epoch 215 | Train Loss: 0.3780 | Val Loss: 0.4897 | Val Macro-F1: 0.9140\n",
      "Epoch 216 | Train Loss: 0.3779 | Val Loss: 0.4867 | Val Macro-F1: 0.9155\n",
      "Epoch 217 | Train Loss: 0.3774 | Val Loss: 0.4861 | Val Macro-F1: 0.9161\n",
      "Epoch 218 | Train Loss: 0.3775 | Val Loss: 0.4896 | Val Macro-F1: 0.9140\n",
      "Epoch 219 | Train Loss: 0.3777 | Val Loss: 0.4859 | Val Macro-F1: 0.9153\n",
      "Epoch 220 | Train Loss: 0.3773 | Val Loss: 0.4869 | Val Macro-F1: 0.9158\n",
      "Epoch 221 | Train Loss: 0.3761 | Val Loss: 0.4906 | Val Macro-F1: 0.9140\n",
      "Epoch 222 | Train Loss: 0.3758 | Val Loss: 0.4923 | Val Macro-F1: 0.9132\n",
      "Epoch 223 | Train Loss: 0.3756 | Val Loss: 0.4873 | Val Macro-F1: 0.9157\n",
      "Epoch 224 | Train Loss: 0.3753 | Val Loss: 0.4890 | Val Macro-F1: 0.9155\n",
      "Epoch 225 | Train Loss: 0.3755 | Val Loss: 0.4887 | Val Macro-F1: 0.9156\n",
      "Epoch 226 | Train Loss: 0.3753 | Val Loss: 0.4928 | Val Macro-F1: 0.9127\n",
      "Epoch 227 | Train Loss: 0.3752 | Val Loss: 0.4877 | Val Macro-F1: 0.9163\n",
      "Epoch 228 | Train Loss: 0.3750 | Val Loss: 0.4882 | Val Macro-F1: 0.9157\n",
      "Epoch 229 | Train Loss: 0.3751 | Val Loss: 0.4893 | Val Macro-F1: 0.9151\n",
      "Epoch 230 | Train Loss: 0.3754 | Val Loss: 0.4958 | Val Macro-F1: 0.9121\n",
      "Epoch 231 | Train Loss: 0.3747 | Val Loss: 0.4873 | Val Macro-F1: 0.9166\n",
      "Epoch 232 | Train Loss: 0.3748 | Val Loss: 0.4945 | Val Macro-F1: 0.9123\n",
      "Epoch 233 | Train Loss: 0.3747 | Val Loss: 0.4993 | Val Macro-F1: 0.9104\n",
      "Epoch 234 | Train Loss: 0.3751 | Val Loss: 0.4872 | Val Macro-F1: 0.9164\n",
      "Epoch 235 | Train Loss: 0.3747 | Val Loss: 0.4880 | Val Macro-F1: 0.9156\n",
      "Epoch 236 | Train Loss: 0.3745 | Val Loss: 0.4936 | Val Macro-F1: 0.9128\n",
      "Epoch 237 | Train Loss: 0.3751 | Val Loss: 0.4907 | Val Macro-F1: 0.9146\n",
      "Epoch 238 | Train Loss: 0.3746 | Val Loss: 0.5026 | Val Macro-F1: 0.9072\n",
      "Epoch 239 | Train Loss: 0.3748 | Val Loss: 0.4924 | Val Macro-F1: 0.9136\n",
      "Epoch 240 | Train Loss: 0.3746 | Val Loss: 0.4910 | Val Macro-F1: 0.9133\n",
      "Epoch 241 | Train Loss: 0.3745 | Val Loss: 0.4893 | Val Macro-F1: 0.9157\n",
      "Epoch 242 | Train Loss: 0.3738 | Val Loss: 0.4896 | Val Macro-F1: 0.9154\n",
      "Epoch 243 | Train Loss: 0.3737 | Val Loss: 0.4896 | Val Macro-F1: 0.9152\n",
      "Epoch 244 | Train Loss: 0.3738 | Val Loss: 0.4912 | Val Macro-F1: 0.9146\n",
      "Epoch 245 | Train Loss: 0.3736 | Val Loss: 0.4882 | Val Macro-F1: 0.9161\n",
      "Epoch 246 | Train Loss: 0.3737 | Val Loss: 0.4880 | Val Macro-F1: 0.9164\n",
      "Epoch 247 | Train Loss: 0.3735 | Val Loss: 0.4911 | Val Macro-F1: 0.9144\n",
      "Epoch 248 | Train Loss: 0.3739 | Val Loss: 0.4948 | Val Macro-F1: 0.9133\n",
      "Epoch 249 | Train Loss: 0.3735 | Val Loss: 0.4885 | Val Macro-F1: 0.9157\n",
      "Epoch 250 | Train Loss: 0.3735 | Val Loss: 0.4902 | Val Macro-F1: 0.9149\n",
      "Epoch 251 | Train Loss: 0.3733 | Val Loss: 0.4898 | Val Macro-F1: 0.9160\n",
      "Epoch 252 | Train Loss: 0.3734 | Val Loss: 0.4882 | Val Macro-F1: 0.9154\n",
      "Epoch 253 | Train Loss: 0.3734 | Val Loss: 0.4896 | Val Macro-F1: 0.9155\n",
      "Epoch 254 | Train Loss: 0.3732 | Val Loss: 0.4907 | Val Macro-F1: 0.9145\n",
      "Epoch 255 | Train Loss: 0.3733 | Val Loss: 0.4886 | Val Macro-F1: 0.9160\n",
      "Epoch 256 | Train Loss: 0.3735 | Val Loss: 0.4882 | Val Macro-F1: 0.9163\n",
      "Epoch 257 | Train Loss: 0.3731 | Val Loss: 0.4882 | Val Macro-F1: 0.9159\n",
      "Epoch 258 | Train Loss: 0.3731 | Val Loss: 0.4903 | Val Macro-F1: 0.9150\n",
      "Epoch 259 | Train Loss: 0.3732 | Val Loss: 0.4973 | Val Macro-F1: 0.9118\n",
      "Epoch 260 | Train Loss: 0.3732 | Val Loss: 0.4898 | Val Macro-F1: 0.9153\n",
      "Epoch 261 | Train Loss: 0.3728 | Val Loss: 0.4885 | Val Macro-F1: 0.9158\n",
      "Epoch 262 | Train Loss: 0.3730 | Val Loss: 0.4888 | Val Macro-F1: 0.9165\n",
      "Epoch 263 | Train Loss: 0.3727 | Val Loss: 0.4893 | Val Macro-F1: 0.9156\n",
      "Epoch 264 | Train Loss: 0.3728 | Val Loss: 0.4889 | Val Macro-F1: 0.9158\n",
      "Epoch 265 | Train Loss: 0.3729 | Val Loss: 0.4898 | Val Macro-F1: 0.9160\n",
      "Epoch 266 | Train Loss: 0.3729 | Val Loss: 0.4883 | Val Macro-F1: 0.9163\n",
      "Epoch 267 | Train Loss: 0.3725 | Val Loss: 0.4880 | Val Macro-F1: 0.9159\n",
      "Epoch 268 | Train Loss: 0.3723 | Val Loss: 0.4907 | Val Macro-F1: 0.9149\n",
      "Epoch 269 | Train Loss: 0.3725 | Val Loss: 0.4880 | Val Macro-F1: 0.9160\n",
      "Epoch 270 | Train Loss: 0.3727 | Val Loss: 0.4895 | Val Macro-F1: 0.9157\n",
      "Epoch 271 | Train Loss: 0.3727 | Val Loss: 0.4886 | Val Macro-F1: 0.9162\n",
      "Epoch 272 | Train Loss: 0.3724 | Val Loss: 0.4887 | Val Macro-F1: 0.9160\n",
      "Epoch 273 | Train Loss: 0.3724 | Val Loss: 0.4888 | Val Macro-F1: 0.9163\n",
      "Epoch 274 | Train Loss: 0.3722 | Val Loss: 0.5013 | Val Macro-F1: 0.9093\n",
      "Epoch 275 | Train Loss: 0.3724 | Val Loss: 0.5012 | Val Macro-F1: 0.9092\n",
      "Epoch 276 | Train Loss: 0.3725 | Val Loss: 0.4897 | Val Macro-F1: 0.9157\n",
      "Epoch 277 | Train Loss: 0.3720 | Val Loss: 0.4898 | Val Macro-F1: 0.9164\n",
      "Epoch 278 | Train Loss: 0.3725 | Val Loss: 0.4884 | Val Macro-F1: 0.9159\n",
      "Epoch 279 | Train Loss: 0.3724 | Val Loss: 0.4951 | Val Macro-F1: 0.9128\n",
      "\n",
      "Early stopping at epoch 279 — no improvement for 80 epochs.\n",
      "\n",
      "Best Val Macro-F1: 0.9170\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "epochs = 1000\n",
    "patience = 80\n",
    "\n",
    "best_f1 = 0.0\n",
    "patience_now = 0\n",
    "train_history = []\n",
    "validation_history = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = critertion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * len(y_batch)\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_history.append(train_loss)\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in validation_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            pred = model(X_batch)\n",
    "            loss = critertion(pred, y_batch)\n",
    "            validation_loss += loss.item() * len(y_batch)\n",
    "            pred_out = pred.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(pred_out)\n",
    "    validation_loss /= len(validation_dataset)\n",
    "    valildation_f1_score = f1_score(y_validation, all_preds, average=\"macro\")\n",
    "    scheduler.step(valildation_f1_score)\n",
    "    validation_history.append(valildation_f1_score)\n",
    "    print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {validation_loss:.4f} | Val Macro-F1: {valildation_f1_score:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if valildation_f1_score > best_f1:\n",
    "        best_f1 = valildation_f1_score\n",
    "        patience_now = 0\n",
    "        torch.save(model.state_dict(), 'artifacts/best_model.pth')\n",
    "        print(f\">>>> New best saved (val macro-F1: {best_f1:.4f})\")\n",
    "    else:\n",
    "        patience_now += 1\n",
    "        if patience_now >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch} — \"f\"no improvement for {patience} epochs.\")\n",
    "            break\n",
    "        \n",
    "# print best score after training \n",
    "print(f\"\\nBest Val Macro-F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7e1a3",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42d60903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Macro-F1: 0.9082  (paper baseline: 0.87)\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('artifacts/best_model.pth', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        logits = model(X_batch)\n",
    "        preds  = logits.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "test_f1 = f1_score(y_test, all_preds, average='macro')\n",
    "print(f\"Test Macro-F1: {test_f1:.4f}  (paper baseline: 0.87)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f64cd9",
   "metadata": {},
   "source": [
    "## save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2902c695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('artifacts/model_config.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'input_dim':   input_dim,\n",
    "        'num_classes': num_classes,\n",
    "        'classes':     list(label_encoder.classes_),\n",
    "        'best_val_f1': best_f1,\n",
    "        'test_f1':     test_f1,\n",
    "    }, f)\n",
    "\n",
    "print(\"Config saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
